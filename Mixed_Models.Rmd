---
title: "Mixed Models"
author: "Pedro Victor Brasil Ribeiro"
date: "2021-11-22 - Last changed in `r Sys.Date()`"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{bm}
   - \usepackage{multirow}
   - \usepackage{float}
output: pdf_document
---

```{r setup, include=FALSE}
library(usethis)
library(tidyverse)
library(bbmle)
library(magrittr)
library(lme4)
library(nlme)
library(kableExtra)
library(lattice)
library(latticeExtra)
dados <- read_table2("https://raw.githubusercontent.com/ranalytics/r-tutorials/master/Edition_2015/Data/RIKZ.txt")
#load("lme_env.RData")
knitr::opts_chunk$set(echo = FALSE)
```

# Longitudinal Data

Longitudinal data, also called as panel data, is data that is collected through a series of repeated observations of the same subject overtime. Longitudinal experiments planning concern the observation of one or more variables in the same subject on different ocassions or condicion of evaluation, time is a commom factor used in most of experiment, but it can be use on distances among other factors, but in the present work it will be implied that the subjects measures is taken over time.

Given that longitudinal data are measures of the same subject taken in a systematic way, is expected not null correlations between the measures, especially the one taken in consecutive. Furthermore is expected heterocedasticidy.

About the data structure is expected 3 characteristics, but have in mind that in a real experiement ambient, not necessarily those characteristics can actually be hold:

\begin{itemize}
  \item Regular (in respect to time): The interval of time between one measure and other are the same;
  \item Balanced (in respect to time): All observations are taken in the same time, on the same condictions in all subjects;
  \item Complete: No lost observations.
\end{itemize}

\begin{table}[H]
\centering
\caption{Basic structure of balanced and complete longitudinal data}
\label{tab1}
\begin{tabular}{cccccc}
\hline
\multirow{2}{*}{Grupo ou Tratamento} & \multirow{2}{*}{Unidade Experimental} & \multicolumn{4}{c}{Condições de Avaliação} \\ \cline{3-6} 
                   &           &  1           & 2           & $\cdots$  & t           \\ \hline
\multirow{4}{*}{1} &  1        &  $y_{111}$   & $y_{112}$   & $\cdots$  & $y_{11t}$   \\
                   &  2        &  $y_{121}$   & $y_{122}$   & $\cdots$  & $y_{12t}$   \\
                   &  $\vdots$ &  $\vdots$    & $\vdots$    & $\ddots$  & $\vdots$    \\
                   &  $n_1$    &  $y_{1n_11}$ & $y_{1n_12}$ & $\cdots$  & $y_{1n_1t}$ \\ \hline
\multirow{4}{*}{2} &  1        &  $y_{211}$   & $y_{212}$   & $\cdots$  & $y_{21t}$   \\
                   &  2        &  $y_{221}$   & $y_{222}$   & $\cdots$  & $y_{22t}$   \\
                   &  $\vdots$ &  $\vdots$    & $\vdots$    & $\ddots$  & $\vdots$    \\
                   &  $n_2$    &  $y_{2n_21}$ & $y_{2n_22}$ & $\cdots$  & $y_{2n_2t}$ \\ \hline
$\vdots$           &  $\vdots$ &  $\vdots$    & $\vdots$    & $\vdots$  & $\vdots$    \\ \hline
\multirow{4}{*}{g} &  1        &  $y_{g11}$   & $y_{g12}$   & $\cdots$  & $y_{g1t}$   \\
                   &  2        &  $y_{g21}$   & $y_{g22}$   & $\cdots$  & $y_{g2t}$   \\
                   &  $\vdots$ &  $\vdots$    & $\vdots$    & $\vdots$  & $\vdots$    \\
                   &  $n_g$    &  $y_{gn_g1}$ & $y_{gn_g2}$ & $\cdots$  & $y_{gn_gt}$ \\ \hline
\end{tabular}
\end{table}

# Mixed Model

For the analysis of longitudinal data to take into account the correlation between measurements taken in the same experimental unit, the adjustment of regression models with the inclusion of random effects can be considered, and it's oftenly used in these kind of cases. The usual models are:

\begin{itemize}
  \item Linear Mixed Models;
  \item Generalized Linear Mixed Models;
  \item Non-Linear Mixed Models.
\end{itemize}

In this we will speak specifically about Linear Mixed Models (LMM) since is the most common model used in this kind of situation. So we'll use LMM to fit the mean profile of the subjects.

As usually done in Linear Models, the LMM is use to make the inference on the populacional mean of the populacion, but in the LMM is used the y conditioned in u, in other words E[Y|u]. For a LMM $X\beta$ represent the fixed effects, but is added a random effect represented by Zu, where just as X is the knew model matriz, and u the random variables used to define the condicional model, expressed as:

\begin{equation}
  E[Y|u] = X\beta + Zu + \varepsilon \label{eq}
\end{equation}

Where from the model \ref{eq} is usually assumed that $b_i \sim N_q(0,G)$ and $\varepsilon \sim N_{m_i}(0,R_i)$, where $m_i$ is the number of instants that the subject is measured.

\begin{table}[H]
\centering
\caption{Summary of the dimensions of each component}
\label{tab1}
\begin{tabular}{lc}
\hline
Componente & Dimensão          \\ \hline
y             & $N \times 1$   \\
X             & $N \times p$   \\
$\beta$       & $p \times 1$   \\
Z             & $N \times nq$  \\
b             & $nq \times 1$  \\
$\varepsilon$ & $N \times 1$   \\
$\Gamma$      & $nq \times nq$ \\
R             & $N \times N$   \\
$\Omega$      & $N \times N$   \\
$\theta$      & $t \times t$   \\ \hline
\end{tabular}
\end{table}

WHere in a more techinicall manner the modelo \ref{eq}, which $y = [ y_1^T, \cdots , y_n^T ] (N \times 1)$, with  $N = \sum_{i = 1}^n n_i$, have the fixed part and the random part, the parameter of the model can be write in a matter where $b \sim N_{nq}(0, \Gamma(\theta))$, where $\Gamma(\theta) = I_n \otimes G(\theta)$ is independent of $\varepsilon \sim N_N(0,R(\theta)), R(\theta) = \otimes_{i = 1}^n R_i(\theta)$, with give the final distribuition of y as $y \sim N_N(X\beta,\Omega(\theta))$, where $\Omega(\theta) = Z\Gamma(\theta)Z^T + R(\theta)$.

Which show us that the fixed effect affect only the mean of y, when the random affect affect the variance of y.

# Structure of the covariance

The greater part of the effort of fit a Mixed Linear Model is to choose the structure of the covariance, which greatly affect the goodness-of-fit of the fitted model.

Theorically speaking those structure can be used on $\Omega(\theta), R(\theta)$ and $G(\theta)$, but is usually used in the variance of y ($\Omega(\theta)$).

Some exemples of covariante structure, considering $n_i = 4$:

## Uniform structure
[$\theta = (\sigma^2,\tau)^T$]

\begin{center}
  $\begin{bmatrix}
   \sigma^2 + \tau & \tau & \tau & \tau \\ 
   \tau & \sigma^2 + \tau & \tau & \tau \\ 
   \tau & \tau & \sigma^2 + \tau & \tau \\ 
   \tau & \tau & \tau & \sigma^2 + \tau
  \end{bmatrix}$
\end{center}

## AR(1) structure
[$\theta = (\sigma^2, \phi)^T$]

\begin{center}
  $\begin{bmatrix}
   1      & \phi   & \phi^2 & \phi^3 \\ 
   \phi   & 1      & \phi   & \phi^2 \\ 
   \phi^2 & \phi   & 1      & \phi \\ 
   \phi^3 & \phi^2 & \phi   & 1
  \end{bmatrix}$
\end{center}

## ARMA(1,1) structure
[$\theta = (\sigma^2,\phi, \gamma)^T$]

\begin{center}
  $\begin{bmatrix}
   1            & \gamma     & \phi\gamma & \gamma\phi^2 \\ 
   \gamma       & 1          & \gamma     & \gamma\phi \\ 
   \gamma\phi   & \gamma     & 1          & \gamma \\ 
   \gamma\phi^2 & \gamma\phi & \gamma     & 1
  \end{bmatrix}$
\end{center}

## Antidependence structure of 1st order
[$\theta = (\sigma_1^2, \sigma_2^2, \sigma_3^2, \sigma_4^2, \rho_1, \rho_2, \rho_3, \rho_4)^T$]

\begin{center}
  $\begin{bmatrix}
   \sigma_1^2            & \sigma_1\sigma_2\rho_1     & \sigma_1\sigma_3\rho_1\rho_2 & \sigma_1\sigma_4\rho_1\rho_2\rho_3 \\ 
   \sigma_1\sigma_2\rho_1       & \sigma_2^2          & \sigma_2\sigma_3\rho_2     & \sigma_2\sigma_4\rho_2\rho_3 \\ 
   \sigma_1\sigma_3\rho_1\rho_2   & \sigma_2\sigma_3\rho_2     & \sigma_3^2          & \sigma_3\sigma_4\rho_3 \\ 
   \sigma_1\sigma_4\rho_1\rho_2\rho_3 & \sigma_2\sigma_4\rho_2\rho_3 & \sigma_3\sigma_4\rho_3     & \sigma_4^2
  \end{bmatrix}$
\end{center}

## Toeplitz structure
[$\theta = (\sigma^2, \sigma_1, \sigma_2, \sigma_3)^T$]

\begin{center}
  $\begin{bmatrix}
   \sigma^2 & \sigma_1 & \sigma_2 & \sigma_3 \\ 
   \sigma_1 & \sigma^2 & \sigma_1 & \sigma_2 \\ 
   \sigma_2 & \sigma_1 & \sigma^2 & \sigma_1 \\ 
   \sigma_3 & \sigma_2 & \sigma_1 & \sigma^2
  \end{bmatrix}$
\end{center}

## Hetorogenally uniform structure
[$\theta = (\sigma_1^2, \sigma_2^2, \sigma_3^2, \sigma_4^2, \rho)^T$]

\begin{center}
  $\begin{bmatrix}
   \sigma_1^2           & \sigma_1\sigma_2\rho & \sigma_1\sigma_3\rho & \sigma_1\sigma_4\rho \\ 
   \sigma_2\sigma_1\rho & \sigma_2^2           & \sigma_2\sigma_3\rho & \sigma_2\sigma_4\rho \\ 
   \sigma_3\sigma_1\rho & \sigma_3\sigma_2\rho & \sigma_3^2           & \sigma_3\sigma_4\rho \\ 
   \sigma_4\sigma_1\rho & \sigma_4\sigma_2\rho & \sigma_4\sigma_3\rho & \sigma_4^2
  \end{bmatrix}$
\end{center}

## Not structured
[$\theta = (\sigma_1^2, \sigma_2^2, \sigma_3^2, \sigma_4^2, \sigma_{12}, \sigma_{13}, \sigma_{14}, \sigma_{23}, \sigma_{24}, \sigma_{34})^T$]

\begin{center}
  $\begin{bmatrix}
   \sigma_1^2  & \sigma_{12} & \sigma_{13} & \sigma_{14} \\ 
   \sigma_{12} & \sigma_2^2  & \sigma_{23} & \sigma_{24} \\ 
   \sigma_{13} & \sigma_{23} & \sigma_3^2  & \sigma_{34} \\ 
   \sigma_{14} & \sigma_{24} & \sigma_{34} & \sigma_4^2
  \end{bmatrix}$
\end{center}

# Chiken diet Data

the `ChickWeight` available on r by using the code `data("ChickWeight")`, is a data that containg 578 row and 4 variable, which measure 50 different chickens on 12 different times where the model contains some datas which some missing value that are not  explicit on the data it self.

For exemple, the Chicken number 18 has only been measure on the time 0 and not on the subsequent time. The help (`help(data("ChickWeight"))`) of the database doesn't provite us the explanation/detail of why that occours.

In the present work, we will be discarting those cases where the data is not complete, as defined in the beginning, since is one of the assumptions of the model, but that problem can be solved by using a different mode, such as Generalized Linear Mixed Models, considering a distribuition capable of fitting data with "censor", or even by using an non-parametric analysis or Random Forest.

But since the presence of NA is only in 5 cases (8,15,16,18,44) was decided to drop the data for lack of good technical features. The absence of which observation can be seen on the table bellow.

```{r, results = 'asis', echo = F, warning = F, message = F}
tab1 <- table(
  ChickWeight$Chick,
  ChickWeight$Time
)
cn <- colnames(tab1)
rn <- rownames(tab1)
tab1 %<>%
  as.matrix.data.frame() %>%
  as_tibble()

tab1 %>% 
  set_colnames(cn) %>%
  summarise(
    across(where(is.numeric), 
           ~ ifelse(
             .x == 0, 'x', .x
           )
    )
  ) %>% 
  cbind(rn) %>% 
  dplyr::select(rn,everything()) %>%  
  rename(Chick = rn) %>% 
  mutate(
    Chick = Chick %>% as.character() %>% as.numeric()
  ) %>% 
  arrange(Chick) %>% 
  kable(format = "latex", booktabs = TRUE) %>%
  kable_styling(
    position = 'center',
    latex_options = 'HOLD_position'
  )
```

Now we'll give some descriptive analisys of the data to help us to get some help for the decision of the implentation of the random effects.

```{r}
data("ChickWeight")
ChickWeight %<>%
  filter(!Chick %in% c(8,15,16,18,44))

frango <- groupedData(
  weight~Time|Chick, outer = ~Diet, data = ChickWeight,
  order.groups = F,
  labels=list(x="tempo (semana)", y="Peso corporal (g)")
)

plot(frango, between = list(y = c(0, 0.5, 0)))
```

The first plot show us a different inclination for each chicken, when taken Weigth in relation with Time, which shown us a possible random effects of the the inclination of the line, but is not shown a possible random effect on the intercepto. By a closer observation on the same plot can be shown a possible quadratic effect, so in the model will be added on the model, and after de decision of the model, we will make a anova to see if that quadratic coef is significative in relation with the model without it.

```{r}
plot(frango, outer = T, key=FALSE) # key omite a legenda
```


By the second graph, it gives us a good visual effect a high variation of the mean weigth for each chicken in time for each Diet, what give us a good intuition that exist a random effect for this case.

```{r}
interaction.plot(frango$Time,frango$Diet,frango$weight,ylab="Peso (g)", xlab="Tempo (semana)")
```


For the last graph, is a good plot to show a possible difference on the effect for each Diet.

```{r, echo = T}
mod1 <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~1,
  data = frango,
  control = lmeControl(opt="optim")
)
mod2 <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~ Diet,
  data = frango,
  control = lmeControl(opt="optim")
)
mod3 <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time+I(Time^2),
  data = frango,
  control = lmeControl(opt="optim")
)
mod4 <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time,
  data = frango,
  control = lmeControl(opt="optim")
)
mod5 <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time + Diet,
  data = frango,
  control = lmeControl(opt="optim")
)
mod6 <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time + Diet + Time:Diet,
  data = frango,
  control = lmeControl(opt="optim")
)
```

So we did the modelling of the models, which can be seen in the code above. We mainly worked with the random effect part using the same fixed effect for them, based of the descriptive analysis that we made. So we made a table showing the AIC and BIC of the fitted models. Where AIC and BIC can be difined as:

\begin{equation}
\begin{aligned}
    AIC =& -2(ln(\hat{L}) - 2k) \\
    BIC =& kln(n) - 2ln(\hat{L}) \label{AICBIC}
\end{aligned}
\end{equation}

```{r}
tibble(
  Modelo = paste('Modelo',1:6),
  AIC = c(AIC(mod1),AIC(mod2),AIC(mod3),AIC(mod4),AIC(mod5),AIC(mod6)),
  BIC = c(BIC(mod1),BIC(mod2),BIC(mod3),BIC(mod4),BIC(mod5),BIC(mod6))
) %>% arrange(BIC,AIC)
```

So the model that gave the lower values for AIC and for BIC the the model 3, which the random effect is added only for the intercept and Time, but looked as a quadradic function. Which lead us to the coeficients on the table bellow:

```{r, results = 'asis'}
coef(mod3) %>%
  as_tibble() %>%
  kable(format = "latex", booktabs = TRUE)
```

It's interesting to note that the estimated coefficients for the intercept and for Time is different for each row, where that row represent each chicken looked in the model (45 chickens), but the others effect are not. Which is why we only added the random effect on these coefficients therefore the coefficients that we did not list, there's no random effect, only fixed effects.

## Changing the correlation matrix structure

Here we'll give a quick looked on who we can add a different correlation structure on the selected models.

```{r, echo = T}
mod3_corAR1 <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time+I(Time^2),
  data = frango,
  control = lmeControl(opt="optim"),
  correlation = corAR1()
)
mod3_corARMA <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time+I(Time^2),
  data = frango,
  control = lmeControl(opt="optim"),
  correlation = corARMA(p = 0, q = 2)
)
mod3_corCAR1 <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time+I(Time^2),
  data = frango,
  control = lmeControl(opt="optim"),
  correlation = corCAR1(form = ~Time)
)
mod3_corCompSymm <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time+I(Time^2),
  data = frango,
  control = lmeControl(opt="optim"),
  correlation = corCompSymm()
)
mod3_corExp <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time+I(Time^2),
  data = frango,
  control = lmeControl(opt="optim"),
  correlation = corExp(form = ~Time)
)
mod3_corGaus <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time+I(Time^2),
  data = frango,
  control = lmeControl(opt="optim"),
  correlation = corGaus(form = ~Time)
)
mod3_corLin <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time+I(Time^2),
  data = frango,
  control = lmeControl(opt="optim"),
  correlation = corLin(form = ~Time)
)
mod3_corRatio <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time+I(Time^2),
  data = frango,
  control = lmeControl(opt="optim"),
  correlation = corRatio(form = ~Time)
)
mod3_corSpher <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time+I(Time^2),
  data = frango,
  control = lmeControl(opt="optim"),
  correlation = corSpher(form = ~ Time)
)
mod3_corSymm <- lme(
  fixed = weight~Time+I(Time^2)+Diet+Time:Diet,
  random = ~Time+I(Time^2),
  data = frango,
  control = lmeControl(opt="optim"),
  correlation = corSymm(form = ~ 1|Chick, fixed = T)
)
```

As said before the selection of the models used can be (and usualy is) a big problem and might require a huge theorical statistcal background, so in these document we'll talk about someways, to select those models, and we'll not enter on the specifics on why the selection of the specific parameters were made.

Because the explanation will depend for the specific structure, but in a nutshell, the selection can be think on how the correlation matrix behave and if it "fits", on the structures shown bellow.

Note that not aways the models converge, problem that is more commom for the correlation structure that can correlate to the "Time Series" problems, such as the AR and ARMA models. That behavior is shown when the roots of the models cannot be but inside a circle on ratio 1. For more details i highly sugest a good read on stationarity and unit roots for ARIMA Process \footnote{Some read sugestion: \href{https://medium.com/analytics-vidhya/a-complete-introduction-to-time-series-analysis-with-r-arma-processes-part-ii-85a6bb5becae}{Link 1} and \href{https://staff.fnwi.uva.nl/p.j.c.spreij/onderwijs/master/aadtimeseries2010.pdf}{Link 2}}

# Interpretation of the models and model selection

```{r}
tibble(
  modelo = c(
    'mod3_corAR1','mod3_corARMA02','mod3_corCAR1',
    'mod3_corCompSymm','mod3_corExp','mod3_corGaus',
    'mod3_corLin','mod3_corRatio','mod3_corSpher',
    'mod3_corSymm'
  ),
  AIC = c(
    AIC(mod3_corAR1),AIC(mod3_corARMA),AIC(mod3_corCAR1),
    AIC(mod3_corCompSymm),AIC(mod3_corExp),AIC(mod3_corGaus),
    AIC(mod3_corLin),AIC(mod3_corRatio),AIC(mod3_corSpher),
    AIC(mod3_corSymm)
  ),
  BIC = c(
    BIC(mod3_corAR1),BIC(mod3_corARMA),BIC(mod3_corCAR1),
    BIC(mod3_corCompSymm),BIC(mod3_corExp),BIC(mod3_corGaus),
    BIC(mod3_corLin),BIC(mod3_corRatio),BIC(mod3_corSpher),
    BIC(mod3_corSymm)
  )
) %>% arrange(AIC,BIC)
```

Using the AIC and BIC values to select the model we have that the lower value is related to "mod3_corRatio", but we can see some problems with the fitted values and the residual of the model.

```{r}
plot(augPred(mod3_corRatio), layout = c(5,3), between = list(y=c(2,1)))
```

In the plot above we can see some cases which the fitted line does not have a good fit, mainly on the cases where the quadratic coefficient could be relevant.

```{r}
plot(mod3_corRatio)
```

In the graph above is made a dispersion point for the fitted values and the standardized residuals, in here we can clearly see a pattern in a shape of a funil on the left side of the plot.

```{r}
plot(mod3_corRatio,Chick~resid(.), abline = 0)
```

Lastly we see some cases which the value 0 is not inside the variation of the boxplot, which is a indication that the coeficient for the specific chicken is not significative.

So for the selected models we do not have good residuals, but why that occours? Since we show that the specific data might have a good response for the Mixed Models, a short answer is that the AIC and BIC values they are not good for the selection of this type of model, since the models have a great ammount of parameters and both values (AIC and BIC) penalized the model for the number of parameters they have, which can be shown in the equation \ref{AICBIC}, they're not good for the selection for Mixed Models \footnote{More details can be seen in Singer \& Rocha (2018), Selection of terms in random coefficient regression models; and Singer, Nobre \& Rocha Análise de dados longitudinais (Portuguese)}.

# Model Comparison

A correlation structure that usually have good results is the Not structured one, som we will used it to make some comparisons with the model selected for the AIC and BIC values. First of all we will make a Nested Model Test.

```{r, echo = T}
anova(mod3_corSymm,mod3_corRatio)
```


```{r}
plot(augPred(mod3_corSymm), layout = c(5,3), between = list(y=c(2,1)))
```

Comparing the fit of the data with the other model we can clearly see a better fit for the data in all chickens, which also means that these model fit the quadratic effect better.

```{r}
plot(mod3_corSymm)
```

For the Fitted Values x Standardized residuals plot, we can see an more random distribution of the point, that are mainly centered in the -2, 2 interval.

```{r}
plot(mod3_corSymm,Chick~resid(.), abline = 0)
```

At last we see that all interval have the value 0, which is a good for the model.

# Conclusion

Given the analysis and all explanation above we can say that the model used all the parameter, plus a quadratic function for time, in the fixed model and the quadradric funtion for time in the random value, with a not structured correlation structure is a good model for fit the weigth of a chicken depending on a Diet that they have.

The structure and theory used in the present document can be used in a great variation of cases that can be seen as a Longitudinal data or the analist have a perception of a presence of a random effect that alter the variance over time (second plot of the descriptive section). As said before i higly sugest a more deep dive on the theory to better understanding of the model, the link and articles suggested can be a good start.

